---
title: "Understanding mixed effects models through simulating data"
author: 'Lisa M. DeBruine & Dale J. Barr'
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(tidyverse)
theme_set(theme_bw())
```


## Abstract

Experimental designs that sample both subjects and stimuli from a larger population need to account for random effects of both subjects and stimuli using mixed effects models. However, much of this research is analyzed using ANOVA on aggregated responses because researchers are not confident specifying and interpreting mixed effects models. The tutorial will explain how to simulate data with random effects structure and analyse the data using linear mixed effects regression (with the lme4 R package). The focus will be on interpreting the LMER output in light of the simulated parameters and comparing the results to by-items and by-subjects ANOVA.

### Prerequisite knowledge or skills

* Basic familiarity with experimental designs where subjects respond to stimuli
* Basic familiarity with R

#### Who will benefit from this tutorial?

Researchers who use experimental designs that need to account for crossed random effects (e.g., designs that sample subjects and stimuli). For example, a large amount of experimental research in face perception or social cognition uses designs that would be better analysed using mixed effects models.

```{r, message=FALSE}
library(tidyverse) # for data wrangling and visualisation
library(afex)      # for LMEM and ANOVA
library(faux)      # devtools::install_github("debruine/faux")
library(broom.mixed) # for extracting data from mixed effect models
set.seed(8675309)  # this makes sure your script uses the same set of random numbers each time you run the full script 
                   # (never set this inside a function or loop)
```

## Pilot Data

In this tutorial we will simulate data from an [Implicit Association Task](https://implicit.harvard.edu/implicit/iatdetails.html). In our example study, people are asked to classify face images as male or female, and words as relating to math or literature. The side of the response options could be *congruent* (male and math on one side, female and literature on the other side) or *incongruent* (female and math on one side, male and literature on the other side). If people have social steoreotypes linking men with math and women with literature, reaction times should be faster for congruent trials than incongruent trials.

### Load the data

We're going to use the `IATData` dataset from the IAT package. We have to do a bit of processing to filter, simplify, and recode this dataset; the code for that is at [github](https://github.com/debruine/sim_mem/blob/master/R/data_prep.R), but you can get the dataset using the code below.

```{r, message=FALSE}

iat_data <- readr::read_csv("iat_data.csv")
#iat_data <- readr::read_csv("https://raw.githubusercontent.com/debruine/sim_mem/master/iat_data.csv")
```

The dataset has the following columns:

* `sub_id`: The subject ID
* `stim_id`: The name of the stimulus
* `condition`: Whether the trial was *congruent* or *incongruent*
* `rt`: Trial latency
* `stim_type`: Whether the stimulus is a *word* or a *face*
* `stim_sex`: Whether the stimulus is stereotyped as *male* or *female*

### Aggregate the data

To start, we'll aggregate the data by subject and condition to calculate mean reaction times. This will create a table of `r n_distinct(iat_data$sub_id)*2` rows with columns `sub_id`, `condition`, and `rt`. 

```{r}
agg_data <- iat_data %>%
  group_by(sub_id, condition) %>%
  summarise(rt = mean(rt)) %>%
  ungroup()
```


### Graph the pilot data

The first step to understanding a new dataset is to plot it. 

```{r pilot-plot}
agg_data %>%
  ggplot(aes(condition, rt, fill = condition)) +
  geom_violin(trim = FALSE, show.legend = FALSE) +
  geom_boxplot(fill = "white", width = 0.2, show.legend = FALSE) +
  scale_fill_manual(values = c("red", "dodgerblue"))
```


### Analyse the data

We'll use a paired-samples t-test to test if mean reaction times in the congruent condition are faster than those in the incongruent condition.

```{r}
t.test(rt~condition, agg_data, paired = TRUE)
```

### Simulate the data

First we put the data into wide format so there is one column for each cell. 

```{r}
vars <- agg_data %>%
  unite(var, condition) %>%
  spread(var, rt) %>%
  select(-sub_id)
```

Then calculate the means and standard deviations for each cell, as well as the correlation between the congruent and incongruent scores. 

```{r, echo = F, results='asis'}
faux::check_sim_stats(vars, digits = 2, usekable = TRUE)
```

```{r}
sub_n <- nrow(vars)
m1 <- mean(vars$congruent)
m2 <- mean(vars$incongruent)
sd1 <- sd(vars$congruent)
sd2 <- sd(vars$incongruent)
r <- cor(vars$congruent, vars$incongruent)
```

You can use the `rnorm_multi()` function from faux to simulate normally-distributed within-subject variables with specified means, SDs and correlations.

```{r}
sim_vars <- faux::rnorm_multi(
  n = sub_n,
  vars = 2,
  cors = r,
  mu = c(m1, m2),
  sd = c(sd1, sd2),
  varnames = c("congruent", "incongruent")
)
```

```{r, echo = F, results='asis'}
faux::check_sim_stats(sim_vars, usekable = TRUE)
```

You can do this quickly using the [`simdf()`](https://debruine.github.io/posts/simdf/) function in faux, which generates a new dataframe from an existing dataframe, simulating all numeric columns from normal distributions with the same mean and SD as the existing data and the same correlation structure as the existing data. However, this method doesn't let you simulate data with different parameters than your pilot data.

```{r, results = 'asis'}
sim_vars <- faux::simdf(vars, n = sub_n)
faux::check_sim_stats(sim_vars, usekable = TRUE)
```

Add a subject ID and put the data back into long format.

```{r}
sim_data <- sim_vars %>%
  mutate(sub_id = 1:sub_n) %>%
  gather(condition, rt, congruent:incongruent)
```

Plot the simulated data along with the pilot data to make sure you did everything correctly.

```{r sim-plot}
bind_rows(
  mutate(agg_data, type = "pilot"),
  mutate(sim_data, type = "simulation")
) %>%
  ggplot(aes(condition, rt, fill = condition)) +
  facet_grid(~type) +
  geom_violin(trim = FALSE, show.legend = FALSE) +
  geom_boxplot(fill = "white", width = 0.2, show.legend = FALSE) +
  scale_fill_manual(values = c("red", "dodgerblue"))
```

### Function

You can wrap this whole procedure in a function so you can easily change the simulation parameters. Simulate the data using the specified parameters, run the t-test, and return a data frame of the test statistics.

```{r}
sim_iat <- function(sub_n, m1, m2, sd1, sd2, r) {
  data <- faux::rnorm_multi(
    n = sub_n,
    vars = 2,
    cors = r,
    mu = c(m1, m2),
    sd = c(sd1, sd2),
    varnames = c("congruent", "incongruent")
  ) %>%
  mutate(sub_id = 1:sub_n) %>%
  gather(condition, rt, congruent:incongruent)
  
  t.test(rt~condition, data, paired = TRUE) %>% tidy()
}
```

### Power calculation

Now you can use this function to run many simulations. There are a lot of ways to do this. The pattern below uses the `map_df()` function from the purrr package. This function takes two arguments, a vector and a function, and returns a dataframe. It runs the function once for each item in the vector, so the vector 1:1000 below runs the function 1000 times.

The effect here is enormous, so can we get away with only testing 20 people? You can replicate this analysis 1000 times with `sub_n = 20` and extract the p-value from the test. Calculate power as the proportion of tests where the p-value is less than your alpha. Here, we'll set alpha to 0.01. We

```{r}
sim <- purrr::map_df(1:1000, ~sim_iat(20, m1, m2, sd1, sd2, r))
alpha <- 0.01
power <- mean(sim$p.value < alpha)
```


```{r, echo = FALSE, fig.width = 4, fig.height = 2.5}
ggplot(sim) + 
  geom_density(aes(estimate), fill = "grey", alpha = .5) +
  geom_vline(xintercept = mean(sim$estimate), color = "red", size = 1.5) +
  #geom_vline(xintercept = quantile(sim$estimate, .25), color = "dodgerblue") +
  #geom_vline(xintercept = quantile(sim$estimate, .75), color = "dodgerblue") +
  ggtitle(paste("mean estimate = ", round(mean(sim$estimate), 2)))
```

```{r, echo = FALSE, fig.width = 4, fig.height = 2.5}
ggplot(sim) + 
  geom_vline(xintercept = alpha, color = "red") +
  geom_density(aes(p.value)) + 
  xlim(0,1) +
  ggtitle(paste("power = ", power))
```

Compare this to the power calculation from `pwr::pwr.t.test()`. This function requires the effect size in Cohen's d, so you can calculate this from the difference score's mean and SD.

```{r}

diff <- vars$incongruent - vars$congruent

pwr::pwr.t.test(
  n = 20,
  type = "paired",
  d = mean(diff)/sd(diff),
  sig.level = alpha
)
```







### ANOVA

See how the ANOVA compares to the t-test. The F-value is the square of the t-value, and the p-values are identical.

```{r, warning=FALSE}
afex::aov_ez(
  id = "sub_id", 
  dv = "rt", 
  within = "condition", 
  data = agg_data
) %>% summary()
```


### LMEM

### Terminology (derived from broom_mixed)

* *terms*: the categorical or continuous predictor variables
* *fixed effects*: the parameters that describe the population-level effects of predictor variables
* *random-effect parameters*: the upper-level parameters that describe the distribution of random variables (variance, covariance, precision, standard deviation, or correlation)
* *random-effect values*: the values that describe the deviation of the observations in a group level from the population-level effect
* *grouping variable*: the categorical variable (factor) that identifies which group or cluster an observation belongs to
* *group level* the particular level of a factor that specifies which level of the grouping variable an observation belongs to



We need to *effect code* condition to match the behaviour of ANOVA. We set the congruent condition to -0.5 and the incongruent condition to +0.5 because we predict lower reaction times for the congruent condition.
```{r}
agg_data$condition.e <- recode(agg_data$condition, 
                               "congruent" = -0.5, 
                               "incongruent" = 0.5)
```

A mixed effects model is specified in this format: `dv ~ terms + (1 | id)`, where `dv` is the value to be predicted, `terms` are the predictor variables, `1` represents the random intercept, and `id` is the grouping variable. Our formula is `rt ~ condition.e + (1 | sub_id)`.

```{r}
lmem <- lmer(rt ~ condition.e + (1 | sub_id), data = agg_data)
summary(lmem)
```

First we'll focus on the fixed effects. Note how the t-values and p-values correspond to the t-test and ANOVA. Also note how the estimate for the effect of condition corresponds to the mean difference calculated in the t-test. The estimate for the intercept corresponds to the overall mean reaction time.

Now we can look at the random effects. You can calculate the proportion of total variance explained by subject-specific intercepts as subject variance divided by total variance. This value ((`r round(10016/(10016 + 8265), 3)`)) is approximately equal to the correlation between the congruent and incongruent conditions (`r round(r, 3)`).


## Simulating Data for LMEM

The data simulation process for a mixed effect model uses the random effect standard deviations and the fixed effect estimates.

### Get parameters from LMEM

The function `broom.mixed::tidy()` helps you to extract important numbers from mixed effects models.

```{r}
params <- broom.mixed::tidy(lmem)
```

`r kable(params)`

Here, we need the grand intercept (`grand_i`), the effect of condition (`cond_eff`), the standard deviation for subject random intercepts (`sub_sd`) and the standard deviation for the residual, or error, variance (`err_sd`)
```{r}

grand_i <- filter(params, effect == "fixed", term == "(Intercept)") %>% pull(estimate)
cond_eff <- filter(params, effect == "fixed", term == "condition.e") %>% pull(estimate)
sub_sd <- filter(params, effect == "ran_pars", group == "sub_id") %>% pull(estimate)
err_sd <- filter(params, effect == "ran_pars", group == "Residual") %>% pull(estimate)
```

Simulate 100 new subjects who have random intercepts with the same standard deviation as the subjects in your pilot data.

```{r}

sub_n <- 100

sub <- tibble(
  sub_id = 1:sub_n,
  sub_i = rnorm(sub_n, 0, sub_sd)
)
```

Create a data table of each trial by crossing the subject IDs with each within-subject condition using `expand.grid()`. This function creates every possible combination of the factors you specify.

```{r}
trials <- expand.grid(
  sub_id = sub$sub_id,
  condition = c("congruent", "incongruent")
)
```

Simulate your data by starting with the trial table, joining in the `sub` table to include each subject's random intercept, effect-coding condition, calculating the error term using the random effects standard deviation for the residual defined above, and calculating the reaction time by adding together the grand intercept (defined above by the intercept fixed effect estimate), the subject-specific intercept, the effect of condition multiplied by the condition effect-code, and the error term.

```{r}
sim_data <- trials %>%
  left_join(sub, by = "sub_id") %>%
  mutate(
    condition.e = recode(condition, "incongruent" = 0.5, "congruent" = -0.5),
    error = rnorm(nrow(.), 0, err_sd),
    rt = grand_i + sub_i + (cond_eff * condition.e) + error
  )
```

Run a mixed-effect model on your simulated data. The `summary()` function is often used to inspect the results.

```{r}
lmer(rt ~ condition.e + (1 | sub_id), data = sim_data) %>%
  summary()
```

### Function

You can wrap this in a function, as well.

```{r}
sim_iat_lme <- function(sub_n = 100, grand_i = 0, 
                        cond_eff = 0, 
                        sub_sd = 1, err_sd = 1) {
  sub <- tibble(
    sub_id = 1:sub_n,
    sub_i = rnorm(sub_n, 0, sub_sd)
  )
  trials <- expand.grid(
    sub_id = sub$sub_id,
    condition = c("congruent", "incongruent")
  )
  sim_data <- trials %>%
    left_join(sub, by = "sub_id") %>%
    mutate(
      condition.e = recode(condition, "incongruent" = 0.5, "congruent" = -0.5),
      error = rnorm(nrow(.), 0, err_sd),
      rt = grand_i + sub_i + (cond_eff * condition.e) + error
    )
  
  lmer(rt ~ condition.e + (1 | sub_id), data = sim_data) %>%  tidy()
}

```

### Power calculation

```{r}

sim <- purrr::map_df(1:100, ~sim_iat_lme(20, grand_i, cond_eff, sub_sd, err_sd))
  
alpha <- 0.01
p <- sim %>%
  filter(term == "condition.e") %>%
  pull(p.value) 

power <- mean(p < alpha)
```

```{r, echo = FALSE, fig.width = 4, fig.height = 2.5}
ggplot() + 
  geom_vline(xintercept = alpha, color = "red") +
  geom_density(aes(p)) + 
  xlim(0,1) +
  ggtitle(paste("power = ", power))

```


## Stop Ignoring Trials

```{r}
iat_data$condition.e <- recode(iat_data$condition, 
                               "congruent" = -0.5, 
                               "incongruent" = 0.5)

pilot_mod <- lmer(rt ~ condition.e +
              (1 + condition.e | sub_id) + 
              (1 | stim_id), 
            data = iat_data)

summary(pilot_mod)
```

### Get parameters from LMEM

```{r}
params <- broom.mixed::tidy(pilot_mod)

grand_i <- filter(params, effect == "fixed", term == "(Intercept)") %>% pull(estimate)
cond_eff <- filter(params, effect == "fixed", term == "condition.e") %>% pull(estimate)
sub_sd <- filter(params, group == "sub_id", term == "sd__(Intercept)") %>% pull(estimate)
stim_sd <- filter(params, group == "stim_id", term == "sd__(Intercept)") %>% pull(estimate)
err_sd <- filter(params, effect == "ran_pars", group == "Residual") %>% pull(estimate)

```

Simulate 100 new subjects who have random intercepts with the same standard deviation as the subjects in your pilot data.

```{r}

sub_n <- 100

sub <- tibble(
  sub_id = 1:sub_n,
  sub_i = rnorm(sub_n, 0, sub_sd)
)
```

### Get the stimuli

```{r}
stim_desc <-  iat_data %>%
  group_by(stim_id, stim_type, stim_sex) %>%
  summarise()

stim <- ranef(pilot_mod)$stim_id %>%
  as_tibble(rownames = 'stim_id') %>%
  rename(stim_i = `(Intercept)`) %>%
  left_join(stim_desc, by = "stim_id")
```

### Graph the stimulus intercepts

```{r}
ggplot(stim, aes(stim_i)) +
  geom_density()
```

Hmm, that looks bimodal.

```{r}
ggplot(stim, aes(stim_i, color = stim_type)) +
  geom_density()
```

### Simulate data

```{r}
sim_dat_lmem <- expand.grid(
  sub_id = sub$sub_id,
  stim_id = stim$stim_id,
  condition = c("congruent", "incongruent")
) %>%
  left_join(sub, by = "sub_id") %>%
  left_join(stim, by = "stim_id") %>%
  mutate(
    condition.e = recode(condition, "incongruent" = 0.5, "congruent" = -0.5),
    stim_sex.e = recode(stim_sex, "male" = 0.5, "female" = -0.5),
    stim_type.e = recode(stim_type, "word" = 0.5, "face" = -0.5),
    error = rnorm(nrow(.), 0, err_sd),
    rt = grand_i + sub_i + stim_i + (cond_eff * condition.e) + error
  )
```



```{r}
sim_mod <- lmer(rt ~ condition.e +
              (1 + condition.e | sub_id) + 
              (1 | stim_id), 
            data = sim_dat_lmem)

summary(sim_mod)
```